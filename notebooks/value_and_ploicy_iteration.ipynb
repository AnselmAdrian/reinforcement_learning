{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports and enviroment setup\n",
    "import numpy as np\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook_connected'\n",
    "\n",
    "import os\n",
    "sns.set()\n",
    "\n",
    "if 'notebook' in os.getcwd():\n",
    "    os.chdir('..')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment setup\n",
    "Here, we will solve a gridworld problem where we can control the terminal locations (where the game ends) and reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create grid world invironment\n",
    "from src.env.gridworld import GridworldEnv, Video_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define the grid world by defining the size and the location of rewards\n",
    "simple_env = np.array([ [ -5,  0,  0,  0,  0, 10],\n",
    "                        [  0,  0,  0,  0,  0,  0],\n",
    "                        [  3,  0,  0, -5,  0,  0],\n",
    "                        [-10,  0,  0,  0,  0,  0]])\n",
    "rewards = simple_env.flatten()\n",
    "terminal_states = [i for i, x in enumerate(rewards) if not (x == 0)] # Terminal states are any location where the reward is not 0\n",
    "\n",
    "env = GridworldEnv(size = simple_env.shape, rewards = rewards, terminal_states = terminal_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.grid_print(env.rewards)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behold, grid world!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A single iteration of value updates\n",
    "For each state, we update the values by checking how each action transitiona the model. For a given state, the value is updated as follows:\n",
    "- For each action given the current state, add the current reward with the discounted reward for every new state it can transition.\n",
    "- Compute the expected reward of the action.\n",
    "- Compute the expected reward over all actions given the policy.\n",
    "\n",
    "$$ V'(s) = \\sum_{a} \\pi(a|s) \\sum_{s', r} Pr(s',r|s,a)(r(s) + \\lambda V_{t}(s'))$$\n",
    "\n",
    "where:\n",
    "- $\\pi$: Policy\n",
    "- $V(s)$: Current value of a state\n",
    "- $V'(s)$: New value of a state\n",
    "- $Pr$: Probability function\n",
    "- $s$: current state\n",
    "- $s'$: next state after an action is taken\n",
    "- $a$: action\n",
    "- $\\lambda$: discount factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_values(env, V, discount_factor=1.0, verbose = 0, delta = 0, callback = None):\n",
    "    V_new = np.copy(V)\n",
    "    if verbose > 0:\n",
    "        env.grid_print(V, k=0)\n",
    "    if callback is not None:\n",
    "        callback.write(np.copy(V_new))\n",
    "    for s in range(env.nS):\n",
    "        q = np.zeros(env.nA)\n",
    "        # Look at the possible next actions\n",
    "        for a in range(env.nA):\n",
    "            # For each action, look at the possible next states\n",
    "            # to calculate q[s,a]\n",
    "            for prob, next_state, reward, done in env.P[s][a]:\n",
    "                # Calculate the value for each action as per backup diagram\n",
    "                if not done:\n",
    "                    q[a] += prob*(reward + discount_factor * V[next_state])\n",
    "                else:\n",
    "                    q[a] += prob * reward\n",
    "\n",
    "        # find the maximum value over all possible actions\n",
    "        # and store updated state value\n",
    "        V_new[s] = q.max()\n",
    "        if verbose > 0:\n",
    "            env.grid_print(V_new, k=s + 1)\n",
    "        if callback is not None:\n",
    "            callback.write(np.copy(V_new))\n",
    "        # How much our value function changed (across any states)\n",
    "        delta = max(delta, np.abs(V_new[s] - V[s]))\n",
    "    return V_new, delta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st Iteration of value updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "video_1 = Video_callback(env.shape, value_range = (rewards.min(), rewards.max()))\n",
    "\n",
    "# Assume we know nothing about the rewards\n",
    "V0 = np.zeros(env.nS)\n",
    "V_new, delta = update_values(env, V0, discount_factor=0.5, verbose = 0, callback = video_1)\n",
    "video_1.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd Interation of vlaue updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_2 = Video_callback(env.shape, value_range = (rewards.min(), rewards.max()))\n",
    "V_new, delta = update_values(env, V_new, discount_factor=0.5, verbose = 0, callback = video_2)\n",
    "video_2.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full value iteration algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration\n",
    "def value_iteration(env, discount_factor=1.0, theta=0.00001, verbose = 0,  callback = None):\n",
    "    \"\"\"\n",
    "    Varry out Value iteration given an environment and a full description\n",
    "    of the environment's dynamics.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI env. env.P -> transition dynamics of the environment.\n",
    "            env.P[s][a] [(prob, next_state, reward, done)].\n",
    "            env.nS is number of states in the environment.\n",
    "            env.nA is number of actions in the environment.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        theta: tolernace level to stop the iterations\n",
    "\n",
    "    Returns:\n",
    "        policy: [S, A] shaped matrix representing optimal policy.\n",
    "        value : [S] length vector representing optimal value\n",
    "    \"\"\"\n",
    "\n",
    "    def argmax_a(arr):\n",
    "        \"\"\"\n",
    "        Return idx of max element in an array.\n",
    "        \"\"\"\n",
    "        max_idx = []\n",
    "        max_val = float('-inf')\n",
    "        for idx, elem in enumerate(arr):\n",
    "            if elem == max_val:\n",
    "                max_idx.append(idx)\n",
    "            elif elem > max_val:\n",
    "                max_idx = [idx]\n",
    "                max_val = elem\n",
    "        return max_idx\n",
    "\n",
    "    optimal_policy = np.zeros([env.nS, env.nA])\n",
    "    V = np.zeros(env.nS)\n",
    "    V_new = np.copy(V)\n",
    "    i = 0\n",
    "    if verbose > 0:\n",
    "        env.grid_print(V_new, k=i)\n",
    "    if callback is not None:\n",
    "        callback.write(np.copy(V_new))\n",
    "\n",
    "    while True:\n",
    "        i += 1\n",
    "        delta = 0\n",
    "        # For each state, perform a \"greedy backup\"\n",
    "        V_new, delta = update_values(env, V, \n",
    "                                     discount_factor=1.0, \n",
    "                                     verbose = verbose - 1, \n",
    "                                     delta = delta)\n",
    "        \n",
    "        V = np.copy(V_new)\n",
    "        if verbose > 0:\n",
    "            env.grid_print(V_new, k=i)\n",
    "        if callback is not None:\n",
    "            callback.write(np.copy(V_new))\n",
    "\n",
    "        # Stop if change is below a threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # V(s) has optimal values. Use these values and one step backup\n",
    "    # to calculate optimal policy\n",
    "    for s in range(env.nS):\n",
    "        q = np.zeros(env.nA)\n",
    "        # Look at the possible next actions\n",
    "        for a in range(env.nA):\n",
    "            # For each action, look at the possible next states\n",
    "            # and calculate q[s,a]\n",
    "            for prob, next_state, reward, done in env.P[s][a]:\n",
    "\n",
    "                # Calculate the value for each action as per backup diagram\n",
    "                if not done:\n",
    "                    q[a] += prob * (reward + discount_factor * V[next_state])\n",
    "                else:\n",
    "                    q[a] += prob * reward\n",
    "\n",
    "        # find the optimal actions\n",
    "        # We are returning stochastic policy which will assign equal\n",
    "        # probability to all those actions which are equal to maximum value\n",
    "        best_actions = argmax_a(q)\n",
    "        optimal_policy[s, best_actions] = 1.0 / len(best_actions)\n",
    "\n",
    "    return optimal_policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_full = Video_callback(env.shape, value_range = (rewards.min(), rewards.max()))\n",
    "# Run policy iteration on Grid world\n",
    "pi_star, V_star = value_iteration(env, verbose = 0, callback = video_full)\n",
    "\n",
    "video_full.plot().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.grid_print(V_star.reshape(env.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Optimal policy\n",
    "print(\"Optimal Policy\\n\", pi_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.plot_policy(V_star, pi_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
